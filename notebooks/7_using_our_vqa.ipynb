{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d17bd415",
   "metadata": {},
   "source": [
    "# Using our VQA system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4a58087-8fd1-4e1f-9913-eeac93a2a4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /Users/wlkim/anaconda3/envs/quick-start-guide-to-llms/lib/python3.10/site-packages (0.17.2)\n",
      "Requirement already satisfied: numpy in /Users/wlkim/anaconda3/envs/quick-start-guide-to-llms/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.2.2 in /Users/wlkim/anaconda3/envs/quick-start-guide-to-llms/lib/python3.10/site-packages (from torchvision) (2.2.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/wlkim/anaconda3/envs/quick-start-guide-to-llms/lib/python3.10/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: filelock in /Users/wlkim/anaconda3/envs/quick-start-guide-to-llms/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/wlkim/anaconda3/envs/quick-start-guide-to-llms/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Users/wlkim/anaconda3/envs/quick-start-guide-to-llms/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/wlkim/anaconda3/envs/quick-start-guide-to-llms/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/wlkim/anaconda3/envs/quick-start-guide-to-llms/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/wlkim/anaconda3/envs/quick-start-guide-to-llms/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/wlkim/anaconda3/envs/quick-start-guide-to-llms/lib/python3.10/site-packages (from jinja2->torch==2.2.2->torchvision) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/wlkim/anaconda3/envs/quick-start-guide-to-llms/lib/python3.10/site-packages (from sympy->torch==2.2.2->torchvision) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e02a6d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import ViTModel, AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, ViTModel, AutoModelForCausalLM\n",
    "import urllib.request\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import ViTModel, ViTFeatureExtractor, AutoTokenizer, AutoModel, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af159176",
   "metadata": {},
   "outputs": [],
   "source": [
    "DECODER_MODEL = 'gpt2'\n",
    "TEXT_ENCODER_MODEL = 'distilbert-base-uncased'\n",
    "IMAGE_ENCODER_MODEL = \"facebook/dino-vitb16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4be09917",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wlkim/anaconda3/envs/quick-start-guide-to-llms/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text_tokenizer = AutoTokenizer.from_pretrained(TEXT_ENCODER_MODEL)\n",
    "decoder_tokenizer = AutoTokenizer.from_pretrained(DECODER_MODEL)\n",
    "decoder_tokenizer.pad_token = decoder_tokenizer.eos_token\n",
    "image_feature_extractor = ViTFeatureExtractor.from_pretrained(IMAGE_ENCODER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d09f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb2b6084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def retrieve_image(image_file):\n",
    "    if 'http' in image_file:\n",
    "        urllib.request.urlretrieve(image_file, \"temp.jpg\")\n",
    "        image_file = 'temp.jpg'\n",
    "    try:\n",
    "        image = Image.open(image_file)\n",
    "        return image\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def preprocess_image(image):\n",
    "    # Open the image if the input is a file path\n",
    "    if type(image) == str:\n",
    "        img = Image.open(image)\n",
    "    else:\n",
    "        img = image\n",
    "\n",
    "    # Check the number of channels in the image and convert to RGB if necessary\n",
    "    if img.mode == 'L':  # 'L' stands for grayscale mode\n",
    "        img_rgb = img.convert('RGB')\n",
    "    else:\n",
    "        img_rgb = img\n",
    "\n",
    "    return img_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d494d092",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, text_encoder_model, image_encoder_model, decoder_model, freeze=None, load_from=None):\n",
    "        super(MultiModalModel, self).__init__()\n",
    "\n",
    "        # Initialize text and image encoders\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_encoder_model)\n",
    "        self.image_encoder = ViTModel.from_pretrained(image_encoder_model)\n",
    "\n",
    "        # Initialize the GPT-2 decoder\n",
    "        self.decoder = AutoModelForCausalLM.from_pretrained(\n",
    "            decoder_model, add_cross_attention=True, tie_word_embeddings=False\n",
    "        )\n",
    "\n",
    "        # Initialize linear layers for projecting encoded features\n",
    "        self.text_projection = nn.Linear(self.text_encoder.config.hidden_size, self.decoder.config.hidden_size)\n",
    "        self.image_projection = nn.Linear(self.image_encoder.config.hidden_size, self.decoder.config.hidden_size)\n",
    "        \n",
    "        # Freeze specified encoders if required\n",
    "        if load_from:\n",
    "            # Load the saved model checkpoint\n",
    "            checkpoint = torch.load(load_from)\n",
    "            checkpoint = {k.replace(\"module.\", \"\"): v for k, v in checkpoint.items()}\n",
    "\n",
    "            # Load the state dictionary into the model\n",
    "            self.load_state_dict(checkpoint)\n",
    "        else:\n",
    "            self.freeze(freeze)\n",
    "\n",
    "    def freeze(self, freeze):\n",
    "        if not freeze:\n",
    "            return\n",
    "\n",
    "        print('Freezing...')\n",
    "        if freeze in ('encoders', 'all') or 'text_encoder' in freeze:\n",
    "            print('Freezing text encoder')\n",
    "            for param in self.text_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if freeze in ('encoders', 'all') or 'image_encoder' in freeze:\n",
    "            print('Freezing image encoder')\n",
    "            for param in self.image_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        if freeze in ('decoder', 'all'):\n",
    "            print('Freezing decoder (except for cross attention)')\n",
    "            for name, param in self.decoder.named_parameters():\n",
    "                if \"crossattention\" not in name:\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    def check_input(self, tensor, tensor_name):\n",
    "        if torch.isnan(tensor).any() or torch.isinf(tensor).any():\n",
    "            print(f\"NaN or infinite values found in {tensor_name}\")\n",
    "\n",
    "    def encode_text(self, input_text, attention_mask):\n",
    "        self.check_input(input_text, \"input_text\")\n",
    "        text_encoded = self.text_encoder(input_text, attention_mask=attention_mask).last_hidden_state.mean(dim=1)\n",
    "        return self.text_projection(text_encoded)\n",
    "\n",
    "    def encode_image(self, input_image):\n",
    "        self.check_input(input_image, \"input_image\")\n",
    "        image_encoded = self.image_encoder(input_image).last_hidden_state.mean(dim=1)\n",
    "        return self.image_projection(image_encoded)\n",
    "\n",
    "    def forward(self, input_text, input_image, decoder_input_ids, attention_mask, labels=None):\n",
    "        self.check_input(decoder_input_ids, \"decoder_input_ids\")\n",
    "\n",
    "        # Encode text and image\n",
    "        text_projected = self.encode_text(input_text, attention_mask)\n",
    "        image_projected = self.encode_image(input_image)\n",
    "\n",
    "        # Combine encoded features\n",
    "        combined_features = (text_projected + image_projected) / 2\n",
    "        if labels is not None:\n",
    "            labels = torch.where(labels == decoder_tokenizer.pad_token_id, -100, labels)\n",
    "\n",
    "        # Decode with GPT-2\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=decoder_input_ids,\n",
    "            labels=labels,\n",
    "            encoder_hidden_states=combined_features.unsqueeze(1)\n",
    "        )\n",
    "        return decoder_outputs\n",
    "\n",
    "    def generate(self, image, questions, max_text_length=5):\n",
    "        # Encode text and image\n",
    "        image = retrieve_image(image)\n",
    "        image_input = image_feature_extractor(images=[preprocess_image(image)], return_tensors=\"pt\")\n",
    "        input_image = image_input[\"pixel_values\"]\n",
    "        image_projected = self.encode_image(input_image)\n",
    "            \n",
    "        \n",
    "        for question in questions:\n",
    "            i = text_tokenizer(question, return_tensors='pt')\n",
    "            text_projected = self.encode_text(i['input_ids'], i['attention_mask'])\n",
    "        \n",
    "\n",
    "            # Combine encoded features\n",
    "            combined_features = (text_projected + image_projected) / 2\n",
    "\n",
    "            generated_so_far = torch.LongTensor([[decoder_tokenizer.bos_token_id]])\n",
    "            with torch.no_grad():\n",
    "                for _ in tqdm(range(max_text_length)):\n",
    "\n",
    "                    decoder_outputs = self.decoder(\n",
    "                        input_ids=generated_so_far,\n",
    "                        encoder_hidden_states=combined_features.unsqueeze(1),\n",
    "                        output_attentions=True\n",
    "                    )\n",
    "\n",
    "                    next_token_logits = decoder_outputs.logits[:, -1, :]\n",
    "                    next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "                    next_token = next_token_logits.argmax(-1)\n",
    "                    confidence = next_token_probs[0, next_token].item()\n",
    "                    print(\"Next token:\", decoder_tokenizer.decode(next_token), \"Confidence:\", confidence)\n",
    "                    generated_so_far = torch.cat((generated_so_far, next_token.unsqueeze(0)), dim=1)\n",
    "            print(question, decoder_tokenizer.decode(generated_so_far[0]))\n",
    "\n",
    "        return image, decoder_outputs, input_image\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35fb88a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/dino-vitb16 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trained_model = MultiModalModel(\n",
    "    image_encoder_model=IMAGE_ENCODER_MODEL, text_encoder_model=TEXT_ENCODER_MODEL,\n",
    "    decoder_model=DECODER_MODEL, load_from=\"vqa_custom/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00d3c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0db45529",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory train2014 is neither a `Dataset` directory nor a `DatasetDict` directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_from_disk\n\u001b[0;32m----> 2\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain2014\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/quick-start-guide-to-llms/lib/python3.10/site-packages/datasets/load.py:2671\u001b[0m, in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, fs, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   2669\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict\u001b[38;5;241m.\u001b[39mload_from_disk(dataset_path, keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[1;32m   2670\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2671\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   2672\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is neither a `Dataset` directory nor a `DatasetDict` directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2673\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Directory train2014 is neither a `Dataset` directory nor a `DatasetDict` directory."
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "val = load_from_disk('vqa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26b769b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[43mval\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      2\u001b[0m v\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val' is not defined"
     ]
    }
   ],
   "source": [
    "v = val[1]\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09289694",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trained_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m image, outputs, processed_image \u001b[38;5;241m=\u001b[39m \u001b[43mtrained_model\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(v[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhat sport is this person playing?\u001b[39m\u001b[38;5;124m'\u001b[39m], max_text_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(processed_image\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trained_model' is not defined"
     ]
    }
   ],
   "source": [
    "image, outputs, processed_image = trained_model.generate(v['image'], ['what sport is this person playing?'], max_text_length=1)\n",
    "plt.imshow(processed_image.squeeze(0).permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b8a9c34",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trained_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m image, outputs, processed_image \u001b[38;5;241m=\u001b[39m \u001b[43mtrained_model\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://upload.wikimedia.org/wikipedia/commons/2/2d/2019-05-18_Fu\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mC3\u001b[39m\u001b[38;5;132;01m%9F\u001b[39;00m\u001b[38;5;124mball\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m2C_Frauen\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m2C_UEFA_Women\u001b[39m\u001b[38;5;132;01m%27s\u001b[39;00m\u001b[38;5;124m_Champions_League\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m2C_Olympique_Lyonnais_-_FC_Barcelona_StP_1192_LR10_by_Stepro\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m28Cropped\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m29.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      3\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhat sport is this person playing?\u001b[39m\u001b[38;5;124m'\u001b[39m], max_text_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(processed_image\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trained_model' is not defined"
     ]
    }
   ],
   "source": [
    "image, outputs, processed_image = trained_model.generate(\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/2/2d/2019-05-18_Fu%C3%9Fball%2C_Frauen%2C_UEFA_Women%27s_Champions_League%2C_Olympique_Lyonnais_-_FC_Barcelona_StP_1192_LR10_by_Stepro%28Cropped%29.jpg', \n",
    "    ['what sport is this person playing?'], max_text_length=1)\n",
    "plt.imshow(processed_image.squeeze(0).permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead7337d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e4eef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09088bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dfd69b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
